{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"devops/ops_prepare/","title":"\u8fd0\u7ef4\u601d\u8003","text":""},{"location":"devops/read_list/","title":"Read list","text":""},{"location":"devops/read_list/#kubernetes-reboot-daemon","title":"Kubernetes Reboot Daemon","text":"<p>https://github.com/weaveworks/kured</p>"},{"location":"devops/read_list/#alarm-web-from-tal-tech","title":"Alarm web from tal-tech","text":"<p>https://github.com/weaveworks/kuredhttps://github.com/tal-tech/alarm-dog</p>"},{"location":"devops/read_list/#admission-hook-solution","title":"Admission hook solution","text":"<p>https://github.com/open-policy-agent/gatekeeper</p>"},{"location":"devops/read_list/#opa-gatekeeper","title":"OPA Gatekeeper","text":"<p>OPA(Open Policy Agent)[https://www.openpolicyagent.org/docs/latest/]: \u5f00\u653e\u7b56\u7565\u4ee3\u7406\uff0c\u5176\u4f7f\u7528\u4e0d\u4ec5\u9650\u4e8e Kubernetes.</p> <p>OPA \u4f7f\u7528 (Rego)[https://www.openpolicyagent.org/docs/latest/policy-language/#what-is-rego] \u4f5c\u4e3a\u7279\u5b9a\u4e8e\u57df\u7684\u8bed\u8a00 </p> <p>OPA vs Gatekeeper (differents)[https://github.com/open-policy-agent/gatekeeper#how-is-gatekeeper-different-from-opa]</p> <p>Gatekeeper: \u7531 OPA \u63d0\u4f9b\u7684\u4e00\u4e2a\u53ef\u81ea\u5b9a\u4e49\u7684 Kubernetes Admission Webhook, \u4f7f\u7528 OPA \u7ea6\u675f\u6846\u67b6\u6765\u63cf\u8ff0\u548c\u5b9e\u65bd\u7b56\u7565\u3002</p> <p>\u53c2\u8003\uff1a</p> <p>(\u9650\u5236\u6761\u4ef6\u6846\u67b6)[https://github.com/open-policy-agent/frameworks/tree/master/constraint]</p> <p>https://cloud.google.com/kubernetes-engine/docs/how-to/pod-security-policies-with-gatekeeper?hl=zh-cn</p> <p>steps:</p> <ol> <li>\u5b9a\u4e49 ConstraintTemplate (\u7ea6\u675f\u6a21\u7248)\uff0c \u5141\u8bb8\u7528\u6237\u58f0\u660e\u65b0\u7684\u7ea6\u675f\uff0c\u662f\u53ef\u91cd\u590d\u4f7f\u7528\u7684\u8bed\u53e5\uff08\u4f7f\u7528 Rego \u7f16\u5199\uff09\uff0c\u53ef\u6839\u636e\u9650\u5236\u6761\u4ef6\u4e2d\u5b9a\u4e49\u7684\u8981\u6c42\u5e94\u7528\u903b\u8f91\u4ee5\u8bc4\u4f30 Kubernetes \u5bf9\u8c61\u4e2d\u7684\u7279\u5b9a\u5b57\u6bb5\u3002 <pre><code>apiVersion: gatekeeper.sh/v1beta1\nkind: ConstraintTemplate\nmetadata:\nname: foosystemrequiredlabels\nspec:\ncrd:\nspec:\nnames:\nkind: FooSystemRequiredLabel\nvalidation:\n# Schema for the `parameters` field\nopenAPIV3Schema:\nproperties:\nlabels:\ntype: array\nitems: string\ntargets:\n- target: admission.k8s.gatekeeper.sh\nlibs:\n# which is a list of all library functions that will be available to the rego package. Note that all packages in libs must have lib as a prefix (e.g. package lib.&lt;something&gt;)\n- |\npackage lib.helpers\n\nmake_message(missing) = msg {\nmsg := sprintf(\"you must provide labels: %v\", [missing])\n}\n\nrego: |\npackage foosystemrequiredlabels\n\nimport data.lib.helpers\n\nviolation[{\"msg\": msg, \"details\": {\"missing_labels\": missing}}] {\nprovided := {label | input.request.object.metadata.labels[label]}\nrequired := {label | label := input.parameters.labels[_]}\nmissing := required - provided\ncount(missing) &gt; 0\nmsg := helpers.make_message(missing)\n}\n</code></pre></li> <li>\u5b9a\u4e49 Constraint (\u9650\u5236\u6761\u4ef6) \u7ea6\u675f\u4e00\u7ec4\u6ee1\u8db3\u8981\u6c42\u7684\u58f0\u660e\uff0c\u662f\u5b89\u5168\u653f\u7b56\u7684\u8868\u793a\u5f62\u5f0f\uff0c\u5b83\u4eec\u5b9a\u4e49\u8981\u6c42\u548c\u5f3a\u5236\u6267\u884c\u8303\u56f4\u3002 <pre><code>apiVersion: constraints.gatekeeper.sh/v1beta1\nkind: FooSystemRequiredLabel\nmetadata:\nname: require-billing-label\nspec:\nmatch:\nnamespace: [\"expensive\"]\nparameters:\nlabels: [\"billing\"]\n</code></pre></li> </ol>"},{"location":"devops/read_list/#admission-webhook","title":"Admission webhook","text":"<p>API HTTP handler \u2192 Authentication \u2192 Mutating admission controllers \u2192 Object schema validation \u2192 Validating admission controllers \u2192 Persisted to ETCD</p>"},{"location":"devops/read_list/#labels-best-practices","title":"Labels Best Practices","text":"<p>https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/ https://www.replex.io/blog/9-additional-best-practices-for-working-with-kubernetes-labels-and-label-selectors</p>"},{"location":"devops/read_list/#spenssl-san-certificate","title":"SpenSSl SAN Certificate","text":"<p>https://liaoph.com/openssl-san/ https://zhuanlan.zhihu.com/p/26646377</p>"},{"location":"devops/read_list/#crossplane-usage-aws","title":"CrossPlane Usage (AWS)","text":"<p>https://aws.amazon.com/cn/blogs/china/connecting-aws-managed-services-to-your-argo-cd-pipeline-with-open-source-crossplane/</p>"},{"location":"devops/read_list/#tcpip","title":"TCP/IP \u8be6\u89e3","text":"<p>https://coolshell.cn/articles/11609.html</p>"},{"location":"devops/read_list/#tcp-udp","title":"TCP UDP","text":"<p>https://mp.weixin.qq.com/s?__biz=MzI0ODk2NDIyMQ==&amp;mid=2247487108&amp;idx=1&amp;sn=7b47f421bb1dee4edb357a10399b7fec&amp;chksm=e999fb96deee7280a17bfff44c27ef11a60e93e48f9da738670a779ecf6accb5a6a4ebd3cbcc&amp;token=933742694&amp;lang=zh_CN#rd </p>"},{"location":"devops/read_list/#gcp-deployment-manaer","title":"GCP Deployment Manaer \u8be6\u89e3","text":"<p>https://cloud.google.com/deployment-manager/docs https://github.com/open-policy-agent/gatekeeper</p>"},{"location":"devops/ci/github_action/actions/action_docker/","title":"How to use Docker to create your own action","text":"<ul> <li>Vist Github</li> <li> <p>Go to hello-world-docker-action and click <code>Use this template</code> button</p> </li> <li> <p>Examples</p> </li> </ul> <p>buildversion-docker-action</p> <p>the main logic of the structure is:</p> <pre><code>.\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 action.yml\n\u251c\u2500\u2500 entrypoint.sh\n</code></pre> <p>workflow should contain <code>action</code> test jobs</p> <pre><code>test: # make sure the action works on a clean machine without building\nruns-on: ubuntu-latest\nsteps:\n- uses: actions/checkout@v2\n- uses: ./\nid: BUILD_VERSION_NUMBER\nwith:\nfile-path: ./version.properties\nrun-number: ${{ github.run_number }}\n- run: echo ${{ steps.BUILD_VERSION_NUMBER.outputs.build_number }}\n</code></pre> <p>!!! note</p> <p>Actions based on Docker should make sure workspace mount path</p>"},{"location":"devops/ci/github_action/actions/action_ts/","title":"How to use <code>ts/javascript</code> to create your own action","text":"<ul> <li>Vist Github</li> <li>Go to typescript-action and click <code>Use this template</code> button</li> </ul> <p>Note</p> <pre><code>.\n\u251c\u2500\u2500 __test__    # testing\n\u251c\u2500\u2500 dist        # build distination folder\n\u251c\u2500\u2500 action.yml  # Action configuration file which contains the entry point for the action\n\u251c\u2500\u2500 src         # Action ts logical code\n</code></pre> <ul> <li>actions/toolkit the code library</li> <li>@actions/core use to return value, logs, secret registration and expose varaibles bewteen actions.s</li> <li>@actions/exec used to exec command</li> <li>@actions/io used to ooperate files.</li> <li>@actions/github GitHub basic</li> </ul> <ul> <li>Install npm dependencies</li> </ul> <pre><code>  npm i &lt;lib&gt;\n</code></pre> <p>Note</p> <p>You can use js-action to generate the template.</p> <ul> <li>Examples</li> </ul> <p>buildversion-action</p> <p>the main logic of the structure is:</p> <pre><code>  ./src\n  \u251c\u2500\u2500 context.ts        # Entry point of the mapping\n\u251c\u2500\u2500 fs-helper.ts      # File handling functions\n\u251c\u2500\u2500 main.ts           # Entry function\n\u251c\u2500\u2500 state-helper.ts   # state management functions\n</code></pre> <p>workflow should contain <code>action</code> test jobs</p> <pre><code>test: # make sure the action works on a clean machine without building\nruns-on: ubuntu-latest\nsteps:\n- uses: actions/checkout@v2\n- uses: ./\nid: BUILD_VERSION_NUMBER\nwith:\nfile-path: ${{ github.workspace }}/__tests__/version.properties\nrun-number: ${{ github.run_number }}\n- run: echo ${{ steps.BUILD_VERSION_NUMBER.outputs.build_number }}\n</code></pre> <p>others ref\uff1asetup-go</p> <p>Note</p> <ul> <li>change <code>scripts</code> -&gt; <code>test</code>: npm run-script build &amp;&amp; jest</li> <li>prettier and vscode ts format conflict issue an be solved by configuring the rules of eslint, you can ask ChatGPT to get detail steps.</li> </ul>"},{"location":"devops/ci/github_action/runtime/kubernetes_runner/","title":"GitHub Action Kubernetes Runner","text":"<p>GitHub Action missing official kubernetes integrated documents\uff0cbut the community has a lot of solutions, here is one of the solution I have tested: actions-runner-controller</p>"},{"location":"devops/ci/github_action/runtime/kubernetes_runner/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>A kubernetes cluster</p> </li> <li> <p>Cert-manager installed in the cluster</p> </li> </ul> <p>Note</p> <p>actions-runner-controller use cert-manager to manage Admission Webhook certificates. Installing cert-manager on Kubernetes</p> <ul> <li>Create organization level runner group</li> </ul> <p>Note</p> <p>Used to limit the visibility of GitHub Runners managing-access-to-self-hosted-runners-using-groups</p> <ul> <li>Create PAT via organization PAT Page</li> </ul> <p>Note</p> <pre><code>https://&lt;GITHUB_ENTERPRISE_URL&gt;/settings/tokens\n</code></pre> <p>Grant access to:</p> <ul> <li>repo (Full control)</li> <li>admin:org (Full control)</li> <li>admin:public_key (read:public_key)</li> <li>admin:repo_hook (read:repo_hook)</li> <li>admin:org_hook (Full control)</li> <li>notifications (Full control)</li> <li>workflow (Full control)</li> </ul> <ul> <li>PAT Test</li> </ul> <p>Run follow commands:</p> <pre><code>curl --header \"Authorization: token &lt;GTIHUB_TOKEN&gt;\" \\\n-X GET \\\n-H \"Accept: application/vnd.github.v3+json\" \\\nhttps://&lt;GITHUB_DOMAIN&gt;/api/v3/orgs/&lt;GITHUB_ORG&gt;/actions/runners/registration-token\n</code></pre>"},{"location":"devops/ci/github_action/runtime/kubernetes_runner/#steps-to-run-self-host-runner-on-kubernetes","title":"Steps to run self-host runner on kubernetes","text":"<ul> <li>Install <code>action-runner-controller</code></li> </ul> <pre><code>helm repo add actions-runner-controller https://actions-runner-controller.github.io/actions-runner-controller\n\n# Follow document to replace values file https://github.com/actions/actions-runner-controller/blob/master/charts/actions-runner-controller/README.md\n# githubEnterpriseServerURL\n# OR\n# githubURL\n# And authSecret.github_token\nhelm upgrade --install --namespace actions-runner-system --create-namespace \\\n--wait actions-runner-controller actions-runner-controller/actions-runner-controller\n</code></pre> <ul> <li>Create <code>Runner</code> <pre><code>apiVersion: actions.summerwind.dev/v1alpha1\nkind: RunnerDeployment\nmetadata:\nname: &lt;GitHub_Org_Name&gt;-runner\nnamespace: &lt;Namespace_Name&gt;\nspec:\nreplicas: 2\ntemplate:\nmetadata:\nannotations:\ncluster-autoscaler.kubernetes.io/safe-to-evict: \"true\"\nspec:\norganization: &lt;GitHub_Org_Name&gt;\nenv: []\ngroup: &lt;GitHub_Group_Name&gt;\nlabels:\n- &lt;GitHub_Org_Name&gt;-runner\n</code></pre></li> </ul>"},{"location":"devops/ci/github_action/runtime/runner_on_ecs/","title":"Github action runner on EC(S|2)","text":"<p>It is also possible to use elastic cumputes to generate runner server on public cloud.</p>"},{"location":"devops/ci/github_action/runtime/runner_on_ecs/#steps","title":"Steps","text":"<ul> <li>Prepare an VPC and its components such as NAT, SG and others.</li> <li>Define metadata/environment variables for actions.</li> <li>Build start up scripts for the server initializing.</li> <li>Create function/lambda to generate elastic cumputes server on public cloud.</li> </ul>"},{"location":"devops/registry/artifactory/","title":"Jfrog CLI","text":""},{"location":"devops/registry/artifactory/#setup-jfrog-cli-config","title":"Setup jfrog cli config","text":"<pre><code># Jfrog server id is \"Artifactory\" alias/name\n$ jfrog config add &lt;SERVER_ID&gt;\n\nJFrog platform URL: &lt;administration -&gt; general -&gt; settings -&gt; general settings |- Custom Base URL &gt;\nJFrog access token (Leave blank for username and password/API key): &lt;leave blank&gt;\nJFrog username: &lt;your artifactory id&gt;\nJFrog password or API key: &lt;edit profile -&gt; generate 'API Key'&gt;\nIs the Artifactory reverse proxy configured to accept a client certificate? (y/n) [n]? y\nClient certificate file path: &lt;leave blank&gt;\nClient certificate key path: &lt;leave blank&gt;\n</code></pre> <pre><code># Test connection\n$ jfrog rt ping\n# or\n$ jfrog rt ping --server-id=&lt;SERVER_ID&gt;\n\n#  if successfull then return 'OK'\n</code></pre>"},{"location":"devops/registry/artifactory/#file-upload","title":"File upload","text":"<pre><code>$ jfrog rt u \"(*).tgz\" &lt;YOUR_TARGET_REPOSITORY_NAME&gt; --server-id=&lt;SERVER_ID&gt;\n</code></pre> <p>Jfrog Administrator Docs</p> <p>Jfrog Cli Docs</p>"},{"location":"kubernetes/schedule_pattern/","title":"Schedule pattern","text":""},{"location":"kubernetes/schedule_pattern/#_1","title":"\u8282\u70b9\u8c03\u5ea6\u89c4\u5219","text":"<ol> <li>Worker Node \u6dfb\u52a0\u6807\u7b7e</li> <li>\u901a\u8fc7 nodeSelector \u548c nodeAffinity \u8bbe\u7f6e\u8c03\u5ea6\u7b56\u7565\uff1a</li> <li>NodeSelector: \u4fdd\u8bc1\u5e94\u7528\u53ea\u8c03\u5ea6\u5230\u5bf9\u5e94\u7684\u8282\u70b9\u4e0a</li> <li>NodeAffinity: \u8c03\u5ea6\u7b56\u7565      |\u7b56\u7565|\u89e3\u91ca|      |---|---|      |requiredDuringSchedulingIgnoredDuringExecution  | \u82e5Pod\u5df2\u90e8\u7f72\u5373\u4f7f\u8282\u70b9\u6807\u7b7e\u53d1\u751f\u53d8\u5316\u4e0d\u518d\u6ee1\u8db3\u6307\u5b9a\u6761\u4ef6\u4e5f\u4f1a\u7ee7\u7eed\u8fd0\u884c.\uff5c      |requiredDuringSchedulingRequiredDuringExecution | \u82e5Pod\u5df2\u90e8\u7f72\u5373\u4f7f\u8282\u70b9\u6807\u7b7e\u53d1\u751f\u53d8\u5316\u4e0d\u518d\u6ee1\u8db3\u6307\u5b9a\u6761\u4ef6\u5219\u91cd\u65b0\u7b97\u5219\u7b26\u5408\u8981\u6c42\u7684\u8282\u70b9.|      |preferredDuringSchedulingIgnoredDuringExecution | <code>\u4f18\u5148</code>\u90e8\u7f72\u5230\u6ee1\u8db3\u6761\u4ef6\u7684\u8282\u70b9\u4e0a\uff0c\u5982\u679c\u4e0d\u6ee1\u8db3\u5219\u5ffd\u7565\u6761\u4ef6\u6309\u6b63\u5e38\u903b\u8f91\u7ee7\u7eed\u90e8\u7f72.|</li> </ol>"},{"location":"kubernetes/knowledge/cpu_limit/","title":"CPU","text":""},{"location":"kubernetes/knowledge/cpu_limit/#_1","title":"\u7b80\u4ecb","text":"<p>Kubernetes \u901a\u8fc7\u5bf9 CPU \u548c RAM\uff0c\u8fd8\u6709\u5176\u4ed6\u8d44\u6e90\u7684\u9650\u5236\uff0c\u53ef\u4ee5\u4fdd\u62a4\u8282\u70b9\u514d\u4e8e\u8017\u5c3d\u8d44\u6e90\u5e76\u53d8\u5f97\u65e0\u54cd\u5e94\u3002</p> <p>\u5185\u5b58\u9650\u5236: \u53ef\u4ee5\u901a\u8fc7\u68c0\u67e5 Pod \u91cd\u542f\u539f\u56e0\u662f\u5426\u4e3a OOMKilled \u6765\u5224\u5b9a\u3002</p> <p>CPU \u9650\u5236: Kubernetes \u4f7f\u7528\u5185\u6838\u8282\u6d41\u548c\u5176\u66b4\u9732\u7684\u6307\u6807\u800c\u4e0d\u662f cgroup \u76f8\u5173\u6307\u6807\uff0c\u8fd9\u5c31\u5f88\u96be\u68c0\u6d4b CPU \u8282\u6d41\u3002\u8fd9\u5c06\u5bfc\u81f4\u5982\u679c\u5e94\u7528\u7a0b\u5e8f\u8d85\u51fa CPU \u9650\u5236\uff0c\u5b83\u5c31\u4f1a\u53d7\u5230\u9650\u5236\u3002</p>"},{"location":"kubernetes/knowledge/cpu_limit/#cpu","title":"CPU \u9650\u5236\u7684\u5fc5\u8981\u6027","text":"<p>\u82e5\u5bb9\u5668\u6ca1\u6709\u6307\u5b9a CPU \u4e0a\u9650\uff0c\u90a3\u5b83\u5c06\u4f7f\u7528\u8be5\u8282\u70b9\u4e0a\u6240\u6709\u53ef\u7528\u7684 CPU\uff0c\u53ef\u80fd\u5bfc\u81f4\u7684\u60c5\u51b5\u662f</p> <ul> <li>\u540c\u8282\u70b9\u5176\u5b83 CPU \u5bc6\u96c6\u578b\u5bb9\u5668\u51cf\u6162\uff0c\u5e76\u53ef\u80fd\u8017\u5c3d\u540c\u8282\u70b9\u4e0a\u7684 CPU \u8d44\u6e90\u3002</li> <li>\u5f71\u54cd kubernetes \u6838\u5fc3\u7ec4\u4ef6\u8fdb\u800c\u9020\u6210\u8282\u70b9\u72b6\u6001\u8f6c\u6362\u4e3a NotReady \u5bfc\u81f4\u5bb9\u5668\u91cd\u65b0\u8c03\u5ea6</li> </ul>"},{"location":"kubernetes/knowledge/cpu_limit/#cpu_1","title":"CPU \u9650\u5236\u7684\u5de5\u4f5c\u539f\u7406","text":"<p>\u901a\u8fc7\u9650\u5236\u786e\u4fdd\u5bb9\u5668 CPU \u8fbe\u5230\u6307\u5b9a\u9650\u5236\u4f1a\u6536\u5230\u8282\u6d41\u3002 Kubernetes \u4f7f\u7528 CFS\uff08\u5b8c\u5168\u516c\u5e73\u8c03\u5ea6\uff09\u914d\u989d\u5bf9\u7a0b\u5e8f\u7684 Pod \u5b9e\u65bd CPU \u9650\u5236\u3002 CFS \u6839\u636e\u65f6\u6bb5\u800c\u4e0d\u662f\u57fa\u4e8e\u53ef\u7528\u7684 CPU \u529f\u7387\u6765\u5904\u7406\u8fdb\u7a0b\u7684 CPU \u8d44\u6e90\u5206\u914d\u3002</p> <p>\u4f7f\u7528\u4ee5\u4e0b\u4e24\u4e2a\u6587\u4ef6:</p> <ul> <li>cpu.cfs_quota_us\uff1a\u4e00\u6bb5\u65f6\u95f4\u5185\u7684\u603b\u53ef\u7528\u8fd0\u884c\u65f6\u95f4[\u4ee5\u5fae\u79d2\u4e3a\u5355\u4f4d]    <pre><code>cat /sys/fs/cgroup/cpu,cpuacct/cpu.cfs_period_us\n</code></pre> <code>100000 us = 100 ms</code></li> <li>cpu.cfs_period_us\uff1a\u5468\u671f\u7684\u957f\u5ea6[\u4ee5\u5fae\u79d2\u4e3a\u5355\u4f4d]   <pre><code>cat /sys/fs/cgroup/cpu,cpuacct/cpu.cfs_quota_us\n</code></pre></li> </ul> <p>\u4e3e\u4f8b\uff1a</p> <p>\u5047\u8bbe\u7a0b\u5e8fA\u9700\u8981200ms\u7684\u5904\u7406\u65f6\u95f4\uff0c\u5f53\u8bbe\u7f6e CPU \u9650\u5236\u4e3a 0.4 CPUs \u5373\u7a0b\u5e8f\u5728\u6bcf100ms\u5468\u671f\u83b7\u5f9740ms\u7684\u8fd0\u884c\u65f6\u95f4\uff0c\u90a3\u4e48\u7a0b\u5e8fA\u9700\u8981\u82b1\u8d39440\u6beb\u79d2\u6765\u5b8c\u6210\u3002</p>"},{"location":"kubernetes/knowledge/cpu_limit/#_2","title":"\u603b\u7ed3","text":"<p>CPU \u9650\u5236\u8bbe\u5b9a\u8981\u8003\u8651:</p> <ul> <li>\u5e94\u7528\u5e94\u8be5\u663e\u793a\u5305\u542b CPU \u9650\u5236\u7684\u8bbe\u5b9a\u3002</li> <li>\u4f9d\u636e\u670d\u52a1 CPU \u7684\u5b9e\u9645\u9700\u6c42\u4f5c\u4e3a limit \u8bbe\u5b9a\u503c\u7684\u53c2\u8003\u3002</li> </ul> <p>\u53c2\u8003\uff1a</p> <p>what-wed-do-to-save-from-the-well-known-k8s-incident</p> <p>cpu-default-namespace</p> <p>assign-cpu-resource</p> <p>understanding-resource-limits-in-kubernetes-cpu-time</p> <p>manage-resources-containers</p>"},{"location":"kubernetes/knowledge/hpa/","title":"Overview","text":"<p>HPA \u53ef\u4ee5\u57fa\u4e8e CPU/MEM \u5229\u7528\u7387\u81ea\u52a8\u6269\u7f29 Deployment, StatefulSet \u4e2d\u7684 Pod \u6570\u91cf, \u540c\u65f6\u4e5f\u53ef\u4ee5\u57fa\u4e8e\u5176\u4ed6\u5e94\u7528\u7a0b\u5e8f\u63d0\u4f9b\u7684\u81ea\u5b9a\u4e49\u5ea6\u91cf\u6307\u6807\u6765\u6267\u884c\u81ea\u52a8\u6269\u7f29\u3002</p>"},{"location":"kubernetes/knowledge/hpa/#hpa","title":"HPA \u4f7f\u7528\u7387\u8ba1\u7b97","text":"<p>\u83b7\u53d6 Pod resource request <pre><code>func calculatePodRequests(pods []*v1.Pod, container string, resource v1.ResourceName) (map[string]int64, error) {\nrequests := make(map[string]int64, len(pods))\nfor _, pod := range pods {\npodSum := int64(0)\nfor _, c := range pod.Spec.Containers {\nif container == \"\" || container == c.Name {\nif containerRequest, ok := c.Resources.Requests[resource]; ok {\npodSum += containerRequest.MilliValue()\n} else {\nreturn nil, fmt.Errorf(\"missing request for %s\", resource)\n}\n}\n}\nrequests[pod.Name] = podSum\n}\nreturn requests, nil\n}\n</code></pre></p> <p>\u8ba1\u7b97\u4f7f\u7528\u7387 <pre><code>// GetResourceUtilizationRatio takes in a set of metrics, a set of matching requests,\n// and a target utilization percentage, and calculates the ratio of\n// desired to actual utilization (returning that, the actual utilization, and the raw average value)\nfunc GetResourceUtilizationRatio(metrics PodMetricsInfo, requests map[string]int64, targetUtilization int32) (utilizationRatio float64, currentUtilization int32, rawAverageValue int64, err error) {\nmetricsTotal := int64(0)\nrequestsTotal := int64(0)\nnumEntries := 0\n\nfor podName, metric := range metrics {\nrequest, hasRequest := requests[podName]\nif !hasRequest {\n// we check for missing requests elsewhere, so assuming missing requests == extraneous metrics\ncontinue\n}\n\nmetricsTotal += metric.Value\nrequestsTotal += request\nnumEntries++\n}\n\n// if the set of requests is completely disjoint from the set of metrics,\n// then we could have an issue where the requests total is zero\nif requestsTotal == 0 {\nreturn 0, 0, 0, fmt.Errorf(\"no metrics returned matched known pods\")\n}\n\ncurrentUtilization = int32((metricsTotal * 100) / requestsTotal)\n\nreturn float64(currentUtilization) / float64(targetUtilization), currentUtilization, metricsTotal / int64(numEntries), nil\n}\n</code></pre></p>"},{"location":"kubernetes/knowledge/hpa/#_1","title":"\u8c03\u7528\u94fe\u8def\uff1a","text":"<p><code>scaleForResourceMappings -&gt; computeReplicasForMetrics -&gt; Scales().Update \u66f4\u65b0\u526f\u672c\u6570</code></p>"},{"location":"kubernetes/knowledge/hpa/#_2","title":"\u8ba1\u7b97\u89c4\u5219:","text":"<p><code>\u671f\u671b\u526f\u672c\u6570 = ceil[\u5f53\u524d\u526f\u672c\u6570 * (\u5f53\u524d\u6307\u6807 / \u671f\u671b\u6307\u6807)]</code> </p>"},{"location":"kubernetes/knowledge/hpa/#_3","title":"\u4e0d\u8db3","text":"<ol> <li>\u5e73\u53f0\u8d44\u6e90\u8d85\u914d\u5bfc\u81f4\u6309\u7167request\u6765\u8ba1\u7b97\u4f7f\u7528\u7387\u4f1a\u8d85\u51fa\u8d44\u6e90\u603b\u91cf\u3002</li> <li>\u53ea\u80fd\u6839\u636e Pod \u7684\u8d44\u6e90\u4f7f\u7528\u7387\u8fdb\u884c\u6269\u5c55\uff0c\u5bf9\u4e8e\u591a\u5bb9\u5668 Pod \u4e0d\u53cb\u597d\u3002</li> <li>\u5355\u7ebf\u7a0b\u5b9e\u73b0\u7684\u6027\u80fd\u95ee\u9898, HPA Source code <pre><code>// Run begins watching and syncing.\nfunc (a *HorizontalController) Run(stopCh &lt;-chan struct{}) {\ndefer utilruntime.HandleCrash()\ndefer a.queue.ShutDown()\n\nklog.Infof(\"Starting HPA controller\")\ndefer klog.Infof(\"Shutting down HPA controller\")\n\nif !cache.WaitForNamedCacheSync(\"HPA\", stopCh, a.hpaListerSynced, a.podListerSynced) {\nreturn\n}\n\n// start a single worker (we may wish to start more in the future)\ngo wait.Until(a.worker, time.Second, stopCh)\n\n&lt;-stopCh\n}\n</code></pre></li> </ol>"},{"location":"kubernetes/knowledge/resource_map/","title":"Resource map","text":""},{"location":"kubernetes/monitor/alert/","title":"Alarm","text":"<p>Use webhook to extend alert channel is flexible and easy to expand.</p>"},{"location":"kubernetes/monitor/alert/#alarm-system-should-contains-following-features","title":"Alarm system should contains following features","text":"<ul> <li>Alarm source management, that is, docking with multiple platforms</li> <li>Alarm channel management, that is, docking multiple channels according to user requirements: Dingtalk, Slack, WeChat, EMail, Phone Call, etc.</li> <li>On-duty management, arrange on-duty personnel according to time slots or certain rules, my understanding is mainly time-based scheduling management.</li> <li>User management, including: contact information, time zone, language, etc.</li> <li>Alarm suppression, grouping, silence function</li> </ul>"},{"location":"kubernetes/monitor/alert/#commercial-solutions","title":"Commercial solutions","text":"<p>Products:</p> <ul> <li>Pagerduty</li> <li>Opsgien</li> <li>ILert</li> </ul> <p>Personal opinion:</p> <ul> <li>Pagerduty has a wide range of users.</li> <li>OPsgien: good integrated with Jira, confluence's products.</li> <li>ILert similar to Pagerduty.</li> </ul>"},{"location":"kubernetes/monitor/alert/#opensource-solutions","title":"Opensource solutions","text":"<ul> <li>Alertmanager</li> <li>Zabbix</li> <li>PrometheusAlert</li> </ul>"},{"location":"kubernetes/monitor/metrics/","title":"\u603b\u7ed3","text":""},{"location":"kubernetes/monitor/metrics/#node","title":"Node \u6307\u6807","text":"Metrics Description More node_cpu_seconds_total Seconds the cpus spent in each mode understanding-machine-cpu-usage node_boot_time_seconds Node boot seconds node_netstat_Tcp_CurrEstab The number of TCP connections whose current status is ESTABLISHED or CLOSE-WAIT node_filesystem_free_bytes Represents the free space ignoring the reserved blocks filesystem-metrics-from-the-node-exporter node_load1 Node persecond Load_(computing) node_load5 Node per 5 seconds Load_(computing) node_load15 Node per 15 seconds Load_(computing) node_filefd_allocated File descriptor statistics: allocated file-descriptor-metrics node_memory_MemTotal_bytes Total memory node_memory_MemAvailable_bytes Available memory (After kernel 3.14) node_uname_info Same as use command <code>uname</code> node_timex_offset_seconds Time offset in between local system and reference clock node_sockstat_TCP_tw The number of something was up in relation to TCP sockets in the TIME_WAIT state analyse-a-metric-by-kernel-version node_sockstat_TCP_alloc The number of allocated (established, applied to sk_buff) TCP sockets node_sockstat_TCP_inuse The number of TCP sockets in use (listening) node_network_transmit_bytes_total Bandwidth usage which comes from the netdev module network-interface-metrics node_netstat_Tcp_PassiveOpens The number of TCP connections that have directly transitioned from the LISTEN state to the SYN-RCVD state node_netstat_Tcp_ActiveOpens Statistic TcpActiveOpens, the number of TCP connections that have directly transitioned from the CLOSED state to the SYN-SENT state node_disk_io_now The number of IOs in progress node_disk node_disk_read_time_seconds_total The total number of milliseconds spent by all reads node_disk node_disk_io_time_seconds_total Total seconds spent doing I/Os node_disk node_disk_written_bytes_total The total number of bytes written successfully node_disk node_disk_write_time_seconds_total This is the total number of seconds spent by all writes node_disk node_disk_writes_completed_total The total number of writes completed successfully node_disk node_disk_io_time_weighted_seconds_total The weighted number of seconds spent doing I/Os. iostats node_disk_reads_completed_total The iostat r/s is the number of reads per second calculated from the previous measurement iostat made (or since boot for the first one) node_disk node_disk_read_bytes_total The total number of bytes read successfully node_disk node_filesystem_readonly Indicates if the filesystem is readonly filesystem-metrics-from-the-node-exporter node_filesystem_size_bytes Total filesystem size filesystem-metrics-from-the-node-exporter node_filesystem_avail_bytes How many bytes are free for use by normal users. filesystem-metrics-from-the-node-exporter node_network_receive_bytes_total Bytes are the base unit"},{"location":"kubernetes/monitor/metrics/#kube-metrics","title":"Kube Metrics","text":"<p>kube-state-metrics-github-docs</p> Metrics Description kube_daemonset_created Unix creation timestamp kube_daemonset_status_current_number_scheduled The number of nodes running at least one daemon pod and are supposed to kube_daemonset_status_desired_number_scheduled The number of nodes that should be running the daemon pod kube_daemonset_status_number_available The number of nodes that should be running the daemon pod and have one or more of the daemon pod running and available kube_daemonset_status_number_misscheduled The number of nodes running a daemon pod but are not supposed to kube_daemonset_status_number_ready The number of nodes that should be running the daemon pod and have one or more of the daemon pod running and ready kube_daemonset_updated_number_scheduled The total number of nodes that are running updated daemon pod kube_deployment_created Unix creation timestamp kube_deployment_metadata_generation Sequence number representing a specific generation of the desired state. kube_deployment_spec_replicas Number of desired pods for a deployment. kube_deployment_spec_strategy_rollingupdate_max_unavailable Maximum number of unavailable replicas during a rolling update of a deployment kube_deployment_status_observed_generation The generation observed by the deployment controller kube_deployment_status_replicas The number of updated replicas per deployment. kube_deployment_status_replicas_available The number of available replicas per deployment. kube_deployment_status_replicas_unavailable The number of unavailable replicas per deployment. kube_deployment_status_replicas_updated The number of updated replicas per deployment. kube_hpa_spec_max_replicas The max value of HPA kube_hpa_spec_min_replicas The min value of HPA kube_hpa_status_condition The condition of HPA kube_hpa_status_current_replicas The status of current replicas kube_hpa_status_desired_replicas The status of desired replicas kube_job_failed The job has failed its execution. kube_job_spec_completions The desired number of successfully finished pods the job should be run with kube_job_status_active The number of actively running pods. kube_job_status_failed The number of pods which reached Phase Failed and the reason for failure. kube_job_status_succeeded The number of pods which reached Phase Succeeded. kube_node_info Information about a cluster node kube_node_spec_taint The taint of a cluster node. kube_node_spec_unschedulable Whether a node can schedule new pods kube_node_status_allocatable_cpu_cores The allocatable cpu cores of a node that are available for scheduling kube_node_status_allocatable_memory_bytes The allocatable memory of a node that are available for scheduling kube_node_status_allocatable_pods The allocatable pods of a node that are available for scheduling kube_node_status_capacity_cpu_cores The cpu cores capacity of a node kube_node_status_capacity_memory_bytes The memory capacity of a node kube_node_status_capacity_pods The pods capacity of a node kube_node_status_condition The condition of a cluster node kube_persistentvolume_status_phase The phase indicates if a volume is available, bound to a claim, or released by a claim. kube_pod_container_info Information about a container in a pod. kube_pod_container_resource_limits_cpu_cores The number of cpu cores requested limit resource by a container. kube_pod_container_resource_limits_memory_bytes The number of memory requested limit resource by a container. kube_pod_container_resource_requests_cpu_cores The number of cpu requested resource by a container. kube_pod_container_resource_requests_memory_bytes The number of memory requested resource by a container. kube_pod_container_status_last_terminated_reason Describes the last reason the container was in terminated state kube_pod_container_status_restarts_total The number of container restarts per container kube_pod_container_status_running Describes whether the container is currently in running state kube_pod_container_status_terminated Describes whether the container is currently in terminated state kube_pod_container_status_terminated_reason Describes the reason the container is currently in terminated state kube_pod_container_status_waiting Describes whether the container is currently in waiting state kube_pod_container_status_waiting_reason Describes the reason the container is currently in waiting state kube_pod_info Information about pod kube_pod_labels Kubernetes labels converted to Prometheus labels kube_pod_owner Information about the Pod's owner kube_pod_status_phase The pods current phase kube_pod_status_ready Describes whether the pod is ready to serve requests kube_service_info Information about service"},{"location":"kubernetes/monitor/metrics/#promtheus-functions","title":"Promtheus functions","text":"Function Description Example predict_linear \u9884\u6d4b Gauge \u6307\u6807\u53d8\u5316\u8d8b\u52bf, \u53ef\u4ee5\u9884\u6d4b\u65f6\u95f4\u5e8f\u5217 v \u5728 t \u79d2\u540e\u7684\u503c\u3002\u5b83\u57fa\u4e8e\u7b80\u5355\u7ebf\u6027\u56de\u5f52\u7684\u65b9\u5f0f\uff0c\u5bf9\u65f6\u95f4\u7a97\u53e3\u5185\u7684\u6837\u672c\u6570\u636e\u8fdb\u884c\u7edf\u8ba1\uff0c\u4ece\u800c\u53ef\u4ee5\u5bf9\u65f6\u95f4\u5e8f\u5217\u7684\u53d8\u5316\u8d8b\u52bf\u505a\u51fa\u9884\u6d4b reduce-noise-from-disk-space-alerts increase \u83b7\u53d6\u533a\u95f4\u5411\u91cf\u4e2d\u7684\u7b2c\u4e00\u4e2a\u540e\u6700\u540e\u4e00\u4e2a\u6837\u672c\u5e76\u8fd4\u56de\u5176\u589e\u957f\u91cf increase(node_cpu[2m]) / 120 \u901a\u8fc7 node_cpu[2m]\u83b7\u53d6\u65f6\u95f4\u5e8f\u5217\u6700\u8fd1\u4e24\u5206\u949f\u7684\u6240\u6709\u6837\u672c\uff0cincrease \u8ba1\u7b97\u51fa\u6700\u8fd1\u4e24\u5206\u949f\u7684\u589e\u957f\u91cf\uff0c\u6700\u540e\u9664\u4ee5\u65f6\u95f4 120 \u79d2\u5f97\u5230 node_cpu \u6837\u672c\u5728\u6700\u8fd1\u4e24\u5206\u949f\u7684\u5e73\u5747\u589e\u957f\u7387 rate \u53ef\u4ee5\u76f4\u63a5\u8ba1\u7b97\u533a\u95f4\u5411\u91cf v \u5728\u65f6\u95f4\u7a97\u53e3\u5185\u5e73\u5747\u589e\u957f\u901f\u7387 rate(node_cpu[2m]) \u548c increase \u8ba1\u7b97\u7684\u5e73\u5747\u589e\u957f\u7387\u76f8\u540c irate \u76f8\u6bd4\u4e8e rate \u51fd\u6570\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u7075\u654f\u5ea6, \u5f53\u9700\u8981\u5206\u6790\u957f\u671f\u8d8b\u52bf\u6216\u8005\u5728\u544a\u8b66\u89c4\u5219\u4e2d\uff0cirate \u7684\u8fd9\u79cd\u7075\u654f\u5ea6\u53cd\u800c\u5bb9\u6613\u9020\u6210\u5e72\u6270\u3002 irate(node_cpu[2m]) histogram_quantile \u7528\u4e8e\u8bc4\u5224\u5f53\u524d\u76d1\u63a7\u6307\u6807\u7684\u670d\u52a1\u6c34\u5e73\uff0c\u4ea7\u751f Histogram \u503c histogram_quantile(0.5, http_request_duration_seconds_bucket) <p>Note</p> <p>rate \u548c increase \u51fd\u6570\u4e0d\u80fd\u53cd\u5e94\u5728\u65f6\u95f4\u7a97\u53e3\u5185\u7684\u5e73\u5747\u589e\u957f\u7387</p>"},{"location":"kubernetes/monitor/solutions/","title":"Monitoring","text":"<p>Prometheus Opertator which is the common solution on Kubernetes, it also support <code>Thonas</code>, there are a lot of articles to show how to use it, easy to understand the whole stack.</p>"},{"location":"kubernetes/monitor/solutions/#prerequisites-of-running-it-locally","title":"Prerequisites of running it locally","text":"<ul> <li>Kubernetes or KinD for local testing.</li> <li>Have selected prometheus stack manifests version for example:</li> </ul> <pre><code>  apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nnamespace: monitoring\n\nresources:\n- ./namespace.yaml\n\nhelmCharts:\n- name: kube-prometheus-stack\nincludeCRDs: true\nreleaseName: kube-prometheus-stack\nversion: 46.8.0\nvaluesFile: values.yaml\nrepo: https://prometheus-community.github.io/helm-charts\n</code></pre> <ul> <li>ArogCD which is very famous GitOps tools to manage Kubernetes deploy. (optional)</li> </ul>"},{"location":"kubernetes/monitor/solutions/#prometheus-operator-work-on-alibaba-cloud","title":"Prometheus operator work on alibaba cloud.","text":"<ul> <li> <p>Work with <code>SLS</code> metric store HA, Cloud native Prometheus solutions and practies.</p> </li> <li> <p><code>Arms Prometheus</code> which work out of box on Ack.</p> </li> </ul>"},{"location":"kubernetes/monitor/solutions/#observability-methods","title":"Observability Methods","text":"<p>USE: the strategies provided by USE, we can quickly locate common performance problems.</p> <ul> <li><code>Utilization</code>: Resource usage, usually expressed as a percentage of a time period, for example, the average memory usage in the last minute is 90%. High usage is often a sign of a system performance bottleneck.</li> <li><code>Saturation</code>: The degree of overloading of resources, tasks that cannot get resources are usually put into the queue, so the queue length or queuing time can be used to measure.</li> <li><code>Errors</code>: The number of error events, which is of particular concern when errors persist and cause performance degradation.</li> </ul> <p>RED: clearly perceive the health of microservice applications and help measure end-user experience issues.</p> <ul> <li> <p><code>(Request) Rate</code>: The number of requests processed per second.</p> </li> <li> <p><code>(Request) Errors</code>: The number of failed requests per second.</p> </li> <li> <p><code>(Request) Duration</code>: Distribution of request processing times.</p> </li> </ul>"},{"location":"kubernetes/monitor/solutions/#issues","title":"Issues","text":"<p>Problem:</p> <ul> <li>metadata.annotations too long issue</li> </ul> <p>Solution steps for apply the helm chart from ArgoCD:</p> <ul> <li> <p>Apply without <code>replace</code> sync options.</p> </li> <li> <p>Select <code>replace</code> sync option if a sync problem occurred like this: \"one or more objects failed to apply, reason: CustomResourceDefinition.apiextensions.k8s.io \"prometheuses.monitoring.coreos.com\" is invalid: metadata.annotations: Too long: must have at most 262144 bytes\"</p> </li> <li> <p>Apply without <code>replace</code> sync option again.</p> </li> </ul>"},{"location":"kubernetes/monitor/solutions/#others","title":"Others","text":"<ul> <li>Kubernetes application publishing and monitoring linkage</li> <li>Prometheus Operator Design</li> <li>Prometheus Operator Concept</li> </ul>"},{"location":"kubernetes/operator/","title":"Overview","text":""},{"location":"public-cloud/","title":"Overview","text":""},{"location":"public-cloud/alibaba_ingress/","title":"Ingress \u90e8\u7f72\u89c4\u5212","text":""},{"location":"public-cloud/alibaba_ingress/#_1","title":"\u57fa\u672c\u6982\u5ff5","text":""},{"location":"public-cloud/alibaba_ingress/#kubernetesservice-ingress-ingress-controller","title":"Kubernetes\u4e2dService, Ingress, Ingress Controller\u7684\u5173\u7cfb","text":"<ul> <li>Service: \u540e\u7aef\u670d\u52a1\u7684\u62bd\u8c61\u3002</li> <li>Ingress: \u53cd\u5411\u4ee3\u7406\u89c4\u5219\u3002</li> <li>Ingress Controller: \u53cd\u54cd\u4ee3\u7406\u7a0b\u5e8f\uff0c\u8d1f\u8d23\u89e3\u6790Ingress\u7684\u53cd\u5411\u4ee3\u7406\u89c4\u5219\uff0c\u5f53Ingress\u53d8\u52a8\u65f6 Ingress Controller \u4f1a\u53ca\u65f6\u66f4\u65b0\u76f8\u5e94\u7684\u8f6c\u53d1\u89c4\u5219\u3002</li> </ul>"},{"location":"public-cloud/alibaba_ingress/#ingress-controller","title":"Ingress Controller\u5de5\u4f5c\u539f\u7406","text":"<p>\u901a\u8fc7API Server\u83b7\u53d6Ingress\u8d44\u6e90\u53d8\u5316\u52a8\u6001\u751f\u6210\u53cd\u5411\u4ee3\u7406\u7a0b\u5e8f\u6240\u9700\u7684\u914d\u7f6e\u6587\u4ef6\u7136\u540e\u91cd\u65b0\u52a0\u8f7d\u751f\u6210\u65b0\u7684\u8def\u7531\u8f6c\u53d1\u89c4\u5219\u3002 [\u53c2\u8003\u963f\u91cc\u4e91\u6587\u6863]</p>"},{"location":"public-cloud/alibaba_ingress/#https","title":"\u514d\u8d39HTTPS\u8bc1\u4e66\u7ba1\u7406","text":"<p>\u4e91\u539f\u58f0\u8bc1\u4e66\u7ba1\u7406\u5f00\u6e90\u5de5\u5177 cert-manager\uff0c\u7528\u4e8e\u5728kubernetes\u96c6\u7fa4\u4e2d\u63d0\u4f9bHTTPS\u8bc1\u4e66\u5e76\u81ea\u52a8\u7eed\u671f\uff0c\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u90e8\u7f72\uff1a[\u53c2\u8003\u963f\u91cc\u4e91\u6587\u6863]</p> <ul> <li> <p>\u90e8\u7f72cert-manager     <pre><code>kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.3.0/cert-manager.yaml\n</code></pre></p> </li> <li> <p>\u521b\u5efaClusterIssuer <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-prod-http01\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n    email: your_email_name@gmail.com  #\u66ff\u6362\u4e3a\u60a8\u7684\u90ae\u7bb1\u540d\u3002\n    privateKeySecretRef:\n      name: letsencrypt-http01\n    solvers:\n    - http01: \n        ingress:\n          class: nginx\nEOF\n</code></pre></p> </li> <li> <p>\u521b\u5efaIngress\u8d44\u6e90\u5bf9\u8c61 <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: ingress-tls\n  annotations:\n    kubernetes.io/ingress.class: \"nginx\"\n    cert-manager.io/cluster-issuer: \"letsencrypt-prod-http01\"\nspec:\n  tls:\n  - hosts:\n    - your_domain_name        # \u66ff\u6362\u4e3a\u60a8\u7684\u57df\u540d\u3002\n    secretName: ingress-tls   \n  rules:\n  - host: your_domain_name    # \u66ff\u6362\u4e3a\u60a8\u7684\u57df\u540d\u3002\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: your_service_name  # \u66ff\u6362\u4e3a\u60a8\u7684\u540e\u7aef\u670d\u52a1\u540d\u3002\n          servicePort: your_service_port  # \u66ff\u6362\u4e3a\u60a8\u7684\u670d\u52a1\u7aef\u53e3\u3002\nEOF\n</code></pre></p> </li> </ul>"},{"location":"public-cloud/alibaba_ingress/#_2","title":"\u9ad8\u53ef\u9760\u90e8\u7f72","text":"<p>\u5373\u4e00\u4e2aIngress\u670d\u52a1\u72ec\u5360\u4e00\u4e2aIngress\u8282\u70b9\u7684\u65b9\u5f0f\uff0c\u907f\u514d\u4e1a\u52a1\u5e94\u7528\u548cIngress\u670d\u52a1\u53d1\u751f\u8d44\u6e90\u62a2\u5360</p> <p>\u901a\u8fc7\u7ee9\u70b9\u6807\u7b7e\u6307\u5b9aIngress controller\u4ec5\u8fd0\u884c\u5728\u4e00\u4e9b\u9ad8\u914d\u8282\u70b9\u4e0a</p> <ul> <li>Node \u6dfb\u52a0 Label <pre><code>kubectl label nodes &lt;NODE_NAME&gt; node-role.kubernetes.io/ingress=\"true\"\n</code></pre></li> </ul> <p>Note</p> <ul> <li>\u6dfb\u52a0\u6807\u7b7e\u7684\u8282\u70b9\u6570\u91cf\u8981\u5927\u4e8e\u7b49\u4e8e\u96c6\u7fa4Pod\u526f\u672c\u6570\uff0c\u4ece\u800c\u907f\u514d\u591a\u4e2aPod\u8fd0\u884c\u5728\u540c\u4e00\u4e2a\u8282\u70b9\u4e0a\u3002</li> <li>\u67e5\u8be2\u7ed3\u679c\u4e2d\uff0c\u82e5ROLES\u7684\u503c\u663e\u793a\u4e3anone\uff0c\u5219\u8868\u793a\u4e3aWorker\u8282\u70b9\u3002</li> <li>Ingress\u670d\u52a1\u90e8\u7f72\u5c3d\u91cf\u9009\u62e9\u5230Worker\u8282\u70b9\u6dfb\u52a0\u6807\u7b7e\u3002</li> </ul> <ul> <li>Deployment \u589e\u52a0 NodeSelector \u914d\u7f6e\uff1a <pre><code>kubectl -n kube-system patch deployment nginx-ingress-controller -p '{\"spec\": {\"template\": {\"spec\": {\"nodeSelector\": {\"node-role.kubernetes.io/ingress\": \"true\"}}}}}'\n</code></pre></li> </ul> <p>\u53e6\u5916\u4e00\u79cd\u901a\u8fc7HPA\u7684\u65b9\u5f0f\u6269\u5c55Pod\u526f\u672c\u6570\u7684\u65b9\u5f0f\u4e0d\u518d\u8be6\u8ff0</p>"},{"location":"public-cloud/alibaba_ingress/#_3","title":"\u9ad8\u8d1f\u8f7d\u90e8\u7f72","text":""},{"location":"public-cloud/alibaba_ingress/#_4","title":"\u786c\u4ef6\u9009\u578b","text":"<p>\u9ad8\u5175\u6cd5\u573a\u666f\u79c0\u5b89\u6cfd\u8d60\u524d\u884cECS\u5b9e\u4f8b\uff1a * \u8ba1\u7b97\u578b\u5b9e\u4f8b: ecs.c6e.9xlarge * \u7f51\u7edc\u578b\u5b9e\u4f8b: ecs.g6e.8xlarge</p>"},{"location":"public-cloud/alibaba_ingress/#kubernetes","title":"Kubernetes \u914d\u7f6e","text":"<ul> <li>\u6dfb\u52a0taint \u548c label\uff0c\u8bbe\u7f6eIngress Pod\u72ec\u5360\u8282\u70b9\u8d44\u6e90     <pre><code>kubectl label nodes $node_name ingress-pod=\"yes\"\nkubectl taint nodes $node_name ingress-pod=\"yes\":NoExecute\n</code></pre></li> <li>\u8bbe\u7f6eCPU Policy \u4e3astatic</li> </ul> <p>Note</p> <p>CPU\u7ba1\u7406\u7b56\u7565</p> <ul> <li>\u63a8\u8350\u8c03\u6574ingress-controller service\u5bf9\u5e94\u7684SLB\u89c4\u683c\u4e3a\u8d85\u5f3a\u578b\uff08slb.s3.large</li> <li>\u63a8\u8350\u96c6\u7fa4\u4f7f\u7528Terway\u7f51\u7edc\u63d2\u4ef6\u53ca\u914d\u7f6e\u72ec\u5360ENI\u3002</li> </ul>"},{"location":"public-cloud/alibaba_ingress/#ingress_1","title":"Ingress \u914d\u7f6e","text":"<ul> <li>Ingress Pod QOS \u4e3a Guaranteed \u7c7b\u578b\u3002</li> <li>\u8bbe\u7f6enginx-ingress-controller container\u7684\u8d44\u6e90\u9650\u5236requests\u548climits: 15 Core 20 GiB\u3002</li> <li>\u8bbe\u7f6einitContainer init-sysctl\u7684\u8d44\u6e90\u9650\u5236requests\u548climits: 100 m 70 MiB\u3002</li> <li>\u5220\u9664Ingress Pod\u4e2d\u7684podAntiAffinity\uff0c\u4f7f\u4e00\u4e2a\u8282\u70b9\u4e0a\u53ef\u8c03\u5ea6\u4e24\u4e2aPod\u3002</li> <li>\u8c03\u6574Deployment Replicas\u6570\u4e3a\u65b0\u589e\u8282\u70b9\u6570\u76842\u500d\u3002</li> <li>\u8bbe\u7f6eConfigMap\u7684worker-processes\u6570\u4e3a15\uff08\u9884\u7559\u90e8\u5206\u7ed9\u7cfb\u7edf\u4f7f\u7528\uff09\u3002</li> <li>\u8c03\u6574ConfigMap\u7684keepalive\u94fe\u63a5\u6700\u5927\u8bf7\u6c42\u6570\u3002</li> </ul> <p>[\u53c2\u8003\u963f\u91cc\u4e91\u6587\u6863]</p>"},{"location":"public-cloud/alibaba_ingress/#_5","title":"\u5176\u5b83:","text":"<p>[Nginx Ingress \u9ad8\u5e76\u53d1\u5b9e\u8df5]</p>"},{"location":"public-cloud/alibaba_kubernetes/","title":"AliCloud Managed Kubernetes","text":""},{"location":"public-cloud/alibaba_kubernetes/#_1","title":"\u7f51\u7edc\u89c4\u5212\uff1a","text":"<p>https://help.aliyun.com/document_detail/86500.html</p>"},{"location":"public-cloud/alibaba_kubernetes/#vpc","title":"\u4e13\u6709\u7f51\u7edc VPC \u7f51\u7edc\u89c4\u5212\uff1a","text":"<p>\u5305\u62ec VPC \u7f51\u6bb5\u89c4\u5212\u548c VSwitch \u7f51\u6bb5\u89c4\u5212:</p> <p>VPC \u7f51\u6bb5\u53ea\u80fd\u4ece 10.0.0.0/8\u3001172.16.0.0/12\u3001192.168.0.0/16 \u4e09\u8005\u5f53\u4e2d\u9009\u62e9.</p> <p>VPC VSwitch \u4e3b\u8981\u7528\u4e8e\u8282\u70b9\u901a\u8baf\uff0c\u5176\u7f51\u6bb5\u6ce8\u610f\u4e8b\u9879\uff1a 1. \u53ea\u80fd\u662f\u5f53\u524d VPC \u7f51\u6bb5\u7684\u5b50\u96c6\uff08\u53ef\u4ee5\u548c VPC \u7f51\u6bb5\u4e00\u6837\u4f46\u4e0d\u80fd\u8d85\u8fc7\uff09\u3002 2. \u4e00\u4e2a VPC \u4e0b\u53ef\u4ee5\u521b\u5efa\u591a\u4e2a VSwitch \u4f46 VSwitch\u4e4b\u95f4\u7684\u7f51\u6bb5\u4e0d\u80fd\u91cd\u5408\u3002 3. VSwitch \u5fc5\u987b \u548c Pod VSwitch \u5728\u540c\u4e00\u53ef\u7528\u533a\u4e0b\u3002</p>"},{"location":"public-cloud/alibaba_kubernetes/#kuberentes","title":"Kuberentes \u7f51\u6bb5\u89c4\u5212\uff1a","text":"<p>\u5305\u62ec Pod CIDR \u548c Service CIDR Pod CIDR \u7528\u4e8e Pod \u901a\u8baf\uff1a Terway \u6ce8\u610f\u4e8b\u9879\uff1a   1. Pod \u865a\u62df\u4ea4\u6362\u673a == VPC Switch\u3002   2. Pod IP \u662f\u4ece VPC Switch \u91cc\u83b7\u53d6\u7684\u3002   3. \u8be5\u7f51\u6bb5\u4e0d\u80fd\u4e0e VSwitch \u548c Service CIDR \u91cd\u53e0\u3002   4. VSwitch \u548c Pod CIDR \u9700\u5728\u540c\u4e00\u53ef\u7528\u533a\u4e0b\u3002    Flannel \u6ce8\u610f\u4e8b\u9879\uff1a   1. \u975eVPC\u4ea4\u6362\u673a\uff0c\u4e3a\u865a\u62df\u7f51\u6bb5\u3002   2. \u4e0d\u80fd\u4e0e VSwitch \u7f51\u6bb5\u91cd\u53e0\u3002   3. \u4e0d\u80fd\u4e0e Service CIDR\u7f51\u6bb5\u91cd\u53e0\u3002   \u4f8b\u5982\uff0cVPC\u7f51\u6bb5\u7528\u7684\u662f172.16.0.0/12\uff0cKubernetes\u7684Pod\u5730\u5740\u6bb5\u5c31\u4e0d\u80fd\u4f7f\u7528172.16.0.0/16\u3001172.17.0.0/16\u7b49\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u5730\u5740\u90fd\u5305\u542b\u5728172.16.0.0/12\u91cc\u3002   </p> <p>Service CIDR \u5bf9\u5e94\u5fae ClusterIP\uff0c\u6bcf\u4e2a Service \u90fd\u6709\u81ea\u5df1\u7684 IP\uff0c\u6ce8\u610f\u4e8b\u9879\uff1a 1. \u53ea\u80fd\u5728 kubernetes \u96c6\u7fa4\u5185\u90e8\u4f7f\u7528\uff0c\u4e0d\u80fd\u5728\u96c6\u7fa4\u5916\u90e8\u4f7f\u7528\u3002 2. \u662f\u4e0d\u80fd\u4e0eVSwitch \u5730\u5740\u6bb5\u91cd\u53e0 3. \u4e0d\u80fd\u548c Pod CIDR \u91cd\u53e0\u3002</p>"},{"location":"public-cloud/alibaba_kubernetes/#_2","title":"\u8282\u70b9\u6c60\u9009\u578b","text":"<p>\u6258\u7ba1\u8282\u70b9\u6c60\u9002\u7528\u573a\u666f\uff1a 1. \u53ea\u5173\u6ce8\u4e0a\u5c42\u5e94\u7528\u5f00\u53d1\uff0c\u4e0d\u5e0c\u671b\u4e3b\u52a8\u8fd0\u7ef4worker\u8282\u70b9\u3002 2. \u9700\u8981\u5feb\u901f\u54cd\u5e94CVE\u5b89\u5168\u6f0f\u6d1e\u3002\u5f53\u65b0\u7684CVE\u53d1\u5e03\u540e\uff0c\u80fd\u591f\u8fc5\u901f\u5347\u7ea7\uff0c\u4ece\u800c\u4fee\u8865\u6f0f\u6d1e\u3002 3. \u5bf9\u5e95\u5c42\u8282\u70b9\u7684\u53d8\u66f4\u4e0d\u654f\u611f\uff0c<code>\u4e1a\u52a1Pod\u5bf9\u8fc1\u79fb\u6709\u8f83\u9ad8\u7684\u5bb9\u5fcd\u5ea6</code>\uff0c\u66f4\u52a0\u5173\u6ce8\u4e1a\u52a1\u7684\u5f39\u6027\u800c\u975e\u4e0d\u53ef\u53d8\u6027\u3002 4. \u9700\u8981\u5347\u7ea7\u8282\u70b9\u4e0a\u7684Docker\u7248\u672c\u53caOS\u955c\u50cf\u7248\u672c\u3002</p>"},{"location":"tools/macos/","title":"MacOS \u672c\u5730\u73af\u5883","text":""},{"location":"tools/macos/#_1","title":"\u5de5\u5177\u5b89\u88c5","text":"<ul> <li> <p>IDE\uff1a</p> <p>vscode, vscode cli, sublime</p> </li> <li> <p>\u6548\u7387\u5de5\u5177:</p> <p>Alfred, Iterm2 </p> <p>oh-my-zsh <pre><code># install\nsh -c \"$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\n</code></pre></p> <p>brew <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre> \u66ff\u6362\u4e2d\u79d1\u5927 brew \u6570\u636e\u6e90</p> <p>Note</p> <p><pre><code>$ brew install go\n==&gt; Searching for similarly named formulae...\nError: No similarly named formulae found.\nError: No available formula or cask with the name \"go\".\nIt was migrated from homebrew/cask to homebrew/core.\n</code></pre> \u89e3\u51b3\u65b9\u6cd5\uff1a <pre><code>BREW_HOME_DIR=$(brew --repo)\n[ -d $BREW_HOME_DIR/Library/Taps/homebrew/homebrew-core ] &amp;&amp; rm -rf $BREW_HOME_DIR/Library/Taps/homebrew/homebrew-core\n</code></pre></p> </li> <li> <p>\u672c\u5730\u5f00\u53d1\u73af\u5883\u5b89\u88c5</p> <p>Docker, JDK</p> <pre><code>mkdir -p $HOME/pconf\ncat &lt;&lt;EOF &gt;&gt;$HOME/pconf/.brew.packages.txt\nsocat\nhtop\ngit\ngo\nnode\nminikube\npython\npwgen\nkubectl\ngnupg\nEOF\n\nbrew install $(&lt;$HOME/pconf/.brew.packages.txt)\n\nsudo pip3 install mkdocs-material\n</code></pre> <p>ZSH \u914d\u7f6e <pre><code># \u521b\u5efa ZSH \u672c\u5730\u914d\u7f6e\u6587\u4ef6\nmkdir -p $HOME/pconf\n\ntouch $HOME/pconf/.zsh.local.secret $HOME/pconf/.zsh.local.env $HOME/pconf/.zsh.local.func\n\ncat &lt;&lt;EOF &gt;&gt;$HOME/pconf/.zsh.local.alias\nalias gst=\"git status\"\nalias gsm=\"git summary\"\nalias gadd='git add'\nalias gdiff='git diff'\nalias gfetch='git fetch'\nalias gremote='git remote -v'\nalias grebase='git rebase'\nalias gbranch='git branch'\nalias gpull=\"git pull origin\"\nalias gpush=\"git push origin\"\nalias gcheckout=\"git checkout\"\nalias glog=\"git log\"\nalias gcommit=\"git commit --verbose -m\"\nalias gmerge=\"git merge\"\nalias gpg_restart='gpgconf --kill gpg-agent'\n\nalias open_ssh_configs='subl $HOME/.ssh/configs'\nalias open_zshrc='subl $HOME/.zshrc'\nalias open_zsh_secret='subl $HOME/pconf/.zsh.local.secret'\nalias open_zsh_env='subl $HOME/pconf/.zsh.local.env'\nalias open_zsh_alias='subl $HOME/pconf/.zsh.local.alias'\nalias open_zsh_func='subl $HOME/pconf/.zsh.local.func'\nEOF\n\n\ncat &lt;&lt;EOF &gt;&gt;$HOME/.zprofile\nexport PATH=\"/Applications/Sublime Text.app/Contents/SharedSupport/bin:$PATH\"\n\nexport PATH=\"$PATH:/opt/homebrew/bin\"\nexport GOPATH=$(go env GOPATH)\nexport GOPROXY=https://goproxy.cn\nexport PATH=\"$GOPATH/bin:/usr/local/bin/python3:$PATH\"\n\n\nGPG_TTY=$(tty)\nexport GPG_TTY\n\nsource $HOME/pconf/.zsh.local.secret\nsource $HOME/pconf/.zsh.local.env\nsource $HOME/pconf/.zsh.local.alias\nsource $HOME/pconf/.zsh.local.func\nEOF\n</code></pre></p> </li> </ul>"},{"location":"tools/macos/#_2","title":"\u5176\u5b83","text":"<ul> <li>\u5b57\u4f53\u4f9d\u8d56\uff1a    Jetbrains Mono    <pre><code># \u4e0b\u8f7d\nhttps://www.jetbrains.com/lp/mono/\n\n# \u5b89\u88c5\nhttps://zhuanlan.zhihu.com/p/143057320\n</code></pre></li> </ul>"},{"location":"tools/mkdoc/","title":"\u4f7f\u7528 mkdocs \u642d\u5efa\u9759\u6001\u7ad9\u70b9","text":"<p>\u5176\u5b9e\u65e0\u8bba\u5b98\u65b9\u8fd8\u662f\u535a\u5ba2\u6587\u7ae0\u90fd\u4ecb\u7ecd\u633a\u591a\u7684\uff0c\u8fd9\u91cc\u5c31\u968f\u7b14\u8bb0\u5f55\u4e00\u4e0b\u5427</p>"},{"location":"tools/mkdoc/#_1","title":"\u51c6\u5907","text":"<p>\u5f00\u59cb\u4e4b\u524d\u8bf7\u81ea\u884c\u9605\u8bfb mkdocs \u5b98\u65b9\u5b66\u4e60\u8d44\u6599\u3002</p>"},{"location":"tools/mkdoc/#_2","title":"\u521b\u5efa\u9879\u76ee","text":"<pre><code>mkdir -p my-site\n\ncd my-site\n\nmkdocs new . </code></pre>"},{"location":"tools/mkdoc/#github-workflows","title":"\u52a0\u5165github workflows","text":"<pre><code>name: CI\n\non:\npush:\nbranches: [ master ]\n\nworkflow_dispatch:\n\njobs:\nbuild:\nruns-on: ubuntu-latest\n\nsteps:\n- uses: actions/checkout@v2\n- uses: actions/setup-python@v2\nwith:\npython-version: 3.x\n- run: pip install mkdocs-material\n- run: mkdocs gh-deploy --force\n</code></pre>"},{"location":"tools/mkdoc/#_3","title":"\u63d0\u4ea4\u4ee3\u7801","text":"<pre><code># github \u4e0a\u81ea\u884c\u521b\u5efa repo \u7136\u540e\u6307\u5b9aremote\ngit remote add origin git@github.com:&lt;username&gt;/&lt;projectname&gt;.git\n\ngit add .\n\ngit commit -m 'init project'\n\ngit push origin -u master\n</code></pre> <p>Note</p> <p>\u8fd9\u91cc\u8bf4\u660e\u4e00\u4e0b\uff0c\u63d0\u4ea4\u540eworkflow\u4f1a\u5728\u672c\u9879\u76ee\u5185\u4ea7\u751f\u4e00\u4e2a <code>gh-deploy</code> \u7684\u65b0\u5206\u652f</p>"},{"location":"tools/mkdoc/#github-page","title":"\u914d\u7f6e Github Page","text":"<p>\u6309\u7167\u56fe\u793a\u987a\u5e8f\u8fdb\u5165 <code>Pages</code> \u914d\u7f6e\u9875\u9762\u6700\u540e\u4fdd\u5b58\u5373\u53ef\u3002 </p>"},{"location":"tools/network_related/","title":"Network related","text":""},{"location":"tools/network_related/#_1","title":"\u57fa\u672c\u529f","text":"<p>IP \u5730\u5740\u7531\u7f51\u7edc\u4f4d\u548c\u4e3b\u673a\u4f4d\u4e24\u90e8\u5206\u7ec4\u6210\uff0c\u901a\u8fc7\u5b50\u7f51\u63a9\u7801\u6765\u533a\u5206\u7f51\u7edc\u4f4d\u548c\u4e3b\u673a\u4f4d\uff0c\u5b50\u7f51\u63a9\u7801\u4e3a1\u7684\u662f\u4e3b\u673a\u4f4d\uff0c0\u7684\u662f\u4e3b\u673a\u4f4d\uff0c\u4f8b\u5982\uff1a225.225.225.0\uff0c \u524d\u4e09\u6bb5\u4f4d\u7f51\u7edc\u4f4d\uff0c\u6700\u540e\u4e00\u6bb5\u4f4d\u4e3b\u673a\u4f4d\u3002</p>"},{"location":"tools/network_related/#ip","title":"\u53ef\u7528 IP \u6bb5\uff1a","text":"<p><code>\u7f51\u7edc\u5730\u5740 \uff5e \u5e7f\u64ad\u5730\u5740</code> \u53bb\u9664\u7f51\u7edc\u5730\u5740\u548c\u5e7f\u64ad\u5730\u5740\u540e\u7684\u8303\u56f4\u3002 expr: [\u7f51\u7edc\u5730\u5740+1, \u5e7f\u64ad\u5730\u5740-1]</p>"},{"location":"tools/network_related/#ip_1","title":"\u7f51\u7edc\u5730\u5740\uff08\u5df2\u77e5 IP \u5730\u5740\u548c\u5b50\u7f51\u63a9\u7801\uff09\uff1a","text":"<p>expr: <code>\u7f51\u7edc\u5730\u5740</code> = <code>IP\u4e8c\u8fdb\u5236\u5730\u5740</code> &amp; <code>\u5b50\u7f51\u63a9\u7801\u4e8c\u8fdb\u5236\u5730\u5740</code></p> <p>\u4f8b\u5982\uff1a \u5df2\u77e5 IP\uff1a222.73.196.18/29\uff0c \u5176\u4e2d <code>/29</code>\u4ee3\u8868\u5b50\u7f51\u63a9\u7801*\u4e8c\u8fdb\u5236*\u4ece\u5de6\u5230\u53f3\u670929\u4e2a1 (\u4e8c\u8fdb\u5236 11111111.11111111.11111111.11111000,\u5341\u8fdb\u5236 (255.255.255.248))\uff0cIP \u5730\u5740\u4e8c\u8fdb\u5236 11011110.01001001.11000100.00010010\uff0c\u5341\u8fdb\u5236 \uff08222.73.196.18\uff09\u5219\u5c06\u5b50\u7f51\u63a9\u7801\u548cIP\u5730\u5740\u4e8c\u8fdb\u5236\u505a*\u4f4d\u4e0e\u8fd0\u7b97* \u5f97\u523011011110.01001001.11000100.00010000,\u8f6c\u6362\u4e3a\u5341\u8fdb\u5236\u540e222.73.196.16 \u5373\u7f51\u7edc\u5730\u5740</p>"},{"location":"tools/network_related/#_2","title":"\u5e7f\u64ad\u5730\u5740\u8ba1\u7b97","text":"<p>expr: set IP \u5730\u5740\u6bb5\u4e8c\u8fdb\u5236\u4e3b\u673a\u4f4d = 1</p> <p>\u4f8b\u5982\uff1a \u5df2\u77e5 IP\uff1a222.73.196.18/29\uff0c\u4e8c\u8fdb\u5236\u4e3a\uff1a11011110.01001001.11000100.00010010\uff0c\u63a9\u7801\u5f97\uff1a 32-29 = 3 =&gt; \u540e\u4e09\u4f4d\u4e3a\u4e3b\u673a\u4f4d\uff0c\u8f6c\u6362\u4e3a1\uff0c\u65b0\u7684\u4e8c\u8fdb\u5236\u4e3a\uff1a11011110.01001001.11000100.00010111\uff0c\u6362\u4e3a\u5341\u8fdb\u5236\uff1a222.73.196.23 \u5373\u4e3a\u5e7f\u64ad\u5730\u5740\u3002</p>"},{"location":"tools/network_related/#_3","title":"\u4e3b\u673a\u53f7\u8ba1\u7b97","text":"<p>expr\uff1a<code>\u4e3b\u673a\u53f7</code> = <code>IP\u4e8c\u8fdb\u5236\u5730\u5740</code> &amp; <code>\u5b50\u7f51\u63a9\u7801\u53d6\u53cd</code></p> <p>\u4f8b\u5982\uff1a \u5df2\u77e5 IP\uff1a222.73.196.18/29\uff0c \u5b50\u7f51\u63a9\u7801\u53d6\u53cd\uff1a 00000000.00000000.00000000.00000111\uff0cIP \u4e8c\u8fdb\u5236: 11011110.01001001.11000100.00010010\uff0c \u4f4d\u4e0e\u8fd0\u7b97\u5f97\uff1a00000000.00000000.00000000.00000010 \u5341\u8fdb\u5236\u4e3a 0.0.0.2\uff0c\u5373\u4e3b\u673a\u53f7\u4e3a 2.</p>"},{"location":"tools/plantuml_vscode/","title":"\u751f\u6210 Go \u7a0b\u5e8fUML","text":""},{"location":"tools/plantuml_vscode/#_1","title":"\u51c6\u5907","text":"<ul> <li>\u672c\u5730\u5b89\u88c5\u9879\u76ee: goplantuml</li> <li>\u5b89\u88c5vscode</li> <li>JDK</li> <li>vscode plantuml \u63d2\u4ef6</li> </ul>"},{"location":"tools/plantuml_vscode/#puml","title":"\u547d\u4ee4\u751f\u6210 puml","text":"<pre><code>goplantuml -recursive ./ &gt; diagram_file_name.puml\n</code></pre>"},{"location":"tools/plantuml_vscode/#_2","title":"\u751f\u6210\u56fe\u7247","text":"<ul> <li>\u5728 <code>workspace</code> \u4e0b\u627e\u5230 <code>*.puml</code> \u6587\u4ef6</li> <li>\u9009\u4e2d\u6587\u4ef6 \u2192 \u83dc\u5355\u4e2d\u9009\u62e9 <code>Export workspace diagram</code> \u2192 png; \u7a0d\u7b49\u7247\u523b\u7b49\u5f85\u56fe\u7247\u6e32\u67d3\u6210\u529f\u63d0\u793a\u3002</li> </ul> <p>Note</p> <ul> <li>\u4fee\u6539 vscode settings.json,\u6700\u540e\u4e00\u884c\u6dfb\u52a0plantuml\u53c2\u6570 <pre><code>\"plantuml.commandArgs\": [\n\"-DPLANTUML_LIMIT_SIZE=20000\"\n],\n</code></pre> \u5f53\u751f\u6210\u7684uml\u5c3a\u5bf8\u5f88\u5927\u7684\u65f6\u5019\u901a\u8fc7\u4fee\u6539\u8be5\u53c2\u6570\u53ef\u4ee5\u907f\u514d\u751f\u6210\u56fe\u7247\u88ab\u622a\u65ad\u7684\u95ee\u9898\u3002</li> </ul>"}]}